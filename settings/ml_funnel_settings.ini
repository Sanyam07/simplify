[general]
verbose = True
pandas_warnings = False
gpu = False
seed = 128
conserve_memory = True
model_type = classifier
label = outcome_reversal
data_to_use = train
compute_hyperparameters = True

[files]
encoding = windows-1252
test_data = True
test_chunk = 5000
use_seed_test_chunk = True
boolean_out = True
import_format = csv
export_format = csv
results_format = csv
experiment_folder = dynamic
export_test_tubes = True

[methods]
steps = scaler, splitter, encoder, interactor, splicer, sampler, model, plotter
scaler = minmax, normalize
splitter = train_test
encoder = target, helmert
interactor = polynomial
sampler = smote
selector = none
model = xgb
plotter = default

[scaler_params]

[splitter_params]
test_size = 0.33
val_size = 0
kfolds = 5
krepeats = 10

[encoder_params]

[interactor_params]

[splicer_params]
include_all = True

[sampler_params]

[custom_params]

[selector_params]
n_features_to_select = 30
step = 1
k = 30
score_func = f_classif
alpha = 0.05
threshold = mean

[search_params]
hyperparameter_search = True
search_method = random
scoring = roc_auc, f1, neg_log_loss
n_iter = 20
cv = 5

[random_forest_params]
n_estimators = 20, 1000
max_depth = 5, 30
max_features = 10, 50
max_leaf_nodes = 5, 10
bootstrap = True
oob_score = True
verbose = 0

[xgb_params]
booster = gbtree
objective = binary:logistic
eval_metric = logloss
silent = True
n_estimators = 100, 1000
max_depth = 5, 30
learning_rate = 0.001, 1.0
subsample = 0.4, 0.6
colsample_bytree = 0.4, 0.6
colsample_bylevel = 0.4, 0.6
min_child_weight = 0.5, 1.5
gamma = 0.0, 0.1
alpha = 0.5, 1

[plotter_params]
data_to_use = test

[results]
metrics = roc_auc, f1, accuracy, balanced_accuracy, brier_score_loss, cohen_kappa, hamming, hinge_loss, jaccard, neg_log_loss, matthews_corrcoef, precision, recall, zero_one
join_predictions = True
join_pred_probs = True

