[general]
verbose = True
seed = 43
conserve_memory = True
parallelize = False
gpu = False
packages = chef, critic

[files]
source_format = csv
interim_format = csv
final_format = csv
analysis_format = csv
file_encoding = windows-1252
float_format = %.4f
test_data = True
test_chunk = 500
random_test_chunk = True
boolean_out = True

[farmer]
almanac_steps = none

[chef]
cookbook_steps = scale, split, encode, mix, cleave, sample, reduce, model
data_to_use = train_test
model_type = classify
label = target
calculate_hyperparameters = True
naming_classes = model, cleave
export_all_recipes = True
search_technique = random
scale_techniques = normalize, minmax
split_techniques = train_test
encode_techniques = target
mix_techniques = polynomial
cleave_techniques = none
sample_techniques = smote, adasyn
reduce_techniques = none
model_techniques = xgboost

[critic]
review_steps = summarize, explain, rank, predict, score
summarize_techniques = default
explain_techniques = all
rank_techniques = all
predict_techniques = default
score_techniques = default

[artist]
canvas_steps = paint
painter = default
animator = default

[summarize]

[explain]
data_to_explain = test
explainers = shap, eli5
shap_model_output = probability

[rank]
importances_techniques = gini, shap, permutation

[predict]
prediction_techniques = gini
join_predictions = True
probability_method = gini
join_probabilities = True

[score]
metrics = roc_auc, f1, accuracy, balanced_accuracy, brier_score_loss, hamming, jaccard, neg_log_loss, matthews_corrcoef, precision, recall, zero_one


[style]
data_to_plot = test
plot_style = fivethirtyeight
plot_font = Franklin Gothic Book
seaborn_style = darkgrid
seaborn_context = paper
seaborn_palette = dark

[paint]
interactions_display = 10
features_display = 20
summary_display = 20
dependency_plots = cleave, top_features
shap_plot_type = dot
comparison_plots = False

[animate]

[scale_parameters]
copy = False
encode = ordinal
strategy = uniform
n_bins = 5

[split_parameters]
test_size = 0.33
val_size = 0
n_splits = 5
shuffle = False

[encode_parameters]

[mix_parameters]

[cleave_parameters]
include_all = True

[sample_parameters]
sampling_strategy = auto

[reduce_parameters]
n_features_to_select = 10
step = 1
score_func = f_classif
alpha = 0.05
threshold = mean

[search_parameters]
n_iter = 50
scoring = roc_auc, f1, neg_log_loss
cv = 5
refit = True

[random_forest_parameters]
n_estimators = 20, 1000
max_depth = 5, 30
max_features = 10, 50
max_leaf_nodes = 5, 10
bootstrap = True
oob_score = True
verbose = 0

[xgboost_parameters]
booster = gbtree
objective = binary:logistic
eval_metric = aucpr
silent = True
n_estimators = 50, 1000
max_depth = 5, 15
learning_rate = 0.001, 0.1
subsample = 0.3
colsample_bytree = 0.3
colsample_bylevel = 0.3
min_child_weight = 0.7, 1.0
gamma = 0.0, 0.2
alpha = 0.0, 0.2

[tensor_flow_parameters]

[baseline_classifier_parameters]
strategy = most_frequent
